"""
LLM Service for Message Summarization
Supports OpenAI-compatible APIs
"""
import asyncio
from typing import Optional, List, Dict
from openai import AsyncOpenAI, OpenAIError
from loguru import logger
from app.config.settings import settings


class LLMService:
    """LLMæœåŠ¡ï¼Œç”¨äºæ¶ˆæ¯æ€»ç»“"""
    
    def __init__(self):
        self.client: Optional[AsyncOpenAI] = None


        self.is_enabled = settings.is_llm_configured
        
        if self.is_enabled:
            self.client = AsyncOpenAI(
                api_key=settings.llm_api_key,
                base_url=settings.llm_base_url
            )
            logger.info(f"LLM Service initialized with base_url: {settings.llm_base_url}, model: {settings.llm_model}")
        else:
            logger.info("LLM Service is disabled (not configured)")
   
    async def summarize_messages(
        self, 
        messages: List[Dict[str, str]],
        context: Optional[str] = None,
        max_tokens: int = 1000
    ) -> Optional[Dict[str, any]]:
        """
        æ€»ç»“æ¶ˆæ¯åˆ—è¡¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯æ¡æ¶ˆæ¯æ ¼å¼ä¸º {"sender": "ç”¨æˆ·å", "text": "æ¶ˆæ¯å†…å®¹", "time": "æ—¶é—´"}
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            {"summary": "æ€»ç»“æ–‡æœ¬", "tokens_used": ä¼°è®¡tokenæ•°} æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        if not self.is_enabled:
            logger.warning("LLM Service not configured, cannot summarize")
            return None
            
        if not messages:
            return {"summary": "æ²¡æœ‰æ¶ˆæ¯éœ€è¦æ€»ç»“", "tokens_used": 0}
        
        try:
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            message_text = "\n".join([
                f"[{msg.get('time', '')}] {msg.get('sender', 'æœªçŸ¥ç”¨æˆ·')}: {msg.get('text', '')}"
                for msg in messages
            ])
            
            # é™åˆ¶æ¶ˆæ¯é•¿åº¦ï¼ˆé¿å…è¶…tokenï¼‰
            max_content_chars = 18000
            if len(message_text) > max_content_chars:
                message_text = message_text[:max_content_chars] + "\n... (æ¶ˆæ¯è¿‡å¤šï¼Œå·²æˆªæ–­)"
            
            # æ„å»ºæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¾¤èŠæ¶ˆæ¯æ€»ç»“åŠ©æ‰‹ï¼Œæ“…é•¿ä»å¤§é‡å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯å¹¶ç»“æ„åŒ–å‘ˆç°ã€‚

**æ ¸å¿ƒä»»åŠ¡**ï¼šåˆ†æç¾¤èŠè®°å½•ï¼Œç”Ÿæˆæ¸…æ™°ã€æœ‰ä»·å€¼çš„æ€»ç»“,è®©ç”¨æˆ·å¿«é€Ÿäº†è§£é”™è¿‡çš„è®¨è®ºå†…å®¹ã€‚

**æ€»ç»“åŸåˆ™**ï¼š
1. **æ™ºèƒ½ç­›é€‰**ï¼šè‡ªåŠ¨å¿½ç•¥é—²èŠã€è¡¨æƒ…ã€æ— æ„ä¹‰çš„çŸ­æ¶ˆæ¯ï¼ˆå¦‚"å“ˆå“ˆ"ã€"å¥½çš„"ã€"+1"ç­‰ï¼‰
2. **ä¸»é¢˜èšåˆ**ï¼šè¯†åˆ«å¹¶å½’ç±»ä¸åŒçš„è®¨è®ºä¸»é¢˜ï¼Œå³ä½¿è¯é¢˜äº¤å‰å‡ºç°ä¹Ÿè¦å‡†ç¡®åˆ†ç»„
3. **äººç‰©è¿½è¸ª**ï¼šæ ‡æ³¨æ¯ä¸ªè¯é¢˜çš„ä¸»è¦å‚ä¸è€…å’Œå…³é”®è´¡çŒ®
4. **ä»·å€¼ä¼˜å…ˆ**ï¼šçªå‡ºé—®é¢˜ã€è§£å†³æ–¹æ¡ˆã€å†³ç­–ã€èµ„æºé“¾æ¥ã€æ—¶é—´èŠ‚ç‚¹ç­‰é«˜ä»·å€¼ä¿¡æ¯

**è¾“å‡ºæ ¼å¼**ï¼š
- ä½¿ç”¨ä¸­æ–‡
- é‡‡ç”¨Markdownæ ¼å¼ï¼ˆbullet pointsç”¨"-"ï¼Œç²—ä½“ç”¨**æ–‡æœ¬**ï¼‰
- ä¸ä½¿ç”¨ä»£ç å—åŒ…è£¹ï¼ˆ```ï¼‰ï¼Œè®©Telegramç›´æ¥æ¸²æŸ“
- æ§åˆ¶åœ¨400å­—ä»¥å†…ï¼Œç¡®ä¿ç®€æ´ä½†ä¿¡æ¯å®Œæ•´

**ç»“æ„æ¨¡æ¿**ï¼š
ğŸ“Š **æ¶ˆæ¯æ¦‚è§ˆ**ï¼šå…±Xæ¡æ¶ˆæ¯ï¼ŒXäººå‚ä¸

ğŸ”¥ **æ ¸å¿ƒè¯é¢˜**
- **[è¯é¢˜1åç§°]**ï¼šç®€è¿°è®¨è®ºå†…å®¹ï¼ˆä¸»è¦å‚ä¸è€…ï¼š@ç”¨æˆ·Aã€@ç”¨æˆ·Bï¼‰
  - å…³é”®ç‚¹1
  - å…³é”®ç‚¹2ï¼ˆå¦‚æœ‰è§£å†³æ–¹æ¡ˆæˆ–ç»“è®ºï¼‰
  
ğŸ’¡ **é‡è¦ä¿¡æ¯**
- èµ„æº/é“¾æ¥/æ–‡ä»¶åˆ†äº«
- å¾…åŠäº‹é¡¹æˆ–å†³ç­–
- æ—¶é—´å®‰æ’

ğŸ‘¥ **æ´»è·ƒæˆå‘˜**ï¼š@ç”¨æˆ·Aï¼ˆä¸»è¦è®¨è®ºXï¼‰ã€@ç”¨æˆ·Bï¼ˆåˆ†äº«äº†Yï¼‰

âš ï¸ **éœ€è¦å…³æ³¨**ï¼šæœªè§£å†³çš„é—®é¢˜æˆ–åç»­äº‹é¡¹ï¼ˆå¦‚æœ‰ï¼‰
"""

            context_info = f"\n\nèƒŒæ™¯ä¿¡æ¯ï¼š{context}" if context else ""
            user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç¾¤èŠè®°å½•å¹¶ç”Ÿæˆç»“æ„åŒ–æ€»ç»“ï¼š

{message_text}

**åˆ†æè¦ç‚¹**ï¼š
1. è¯†åˆ«å‡ºæ‰€æœ‰ä¸åŒçš„è®¨è®ºä¸»é¢˜ï¼ˆæŠ€æœ¯é—®é¢˜ã€æ–¹æ¡ˆè®¨è®ºã€èµ„æºåˆ†äº«ã€æ—¥å¸¸äº¤æµç­‰ï¼‰
2. è¿‡æ»¤æ‰çº¯é—²èŠã€é‡å¤ç¡®è®¤ã€æ— å®è´¨å†…å®¹çš„æ¶ˆæ¯
3. æ ‡æ³¨æ¯ä¸ªè¯é¢˜çš„å…³é”®å‚ä¸è€…
4. æå–å¯æ‰§è¡Œä¿¡æ¯ï¼ˆé“¾æ¥ã€æ—¶é—´ã€å¾…åŠç­‰ï¼‰
5. å¦‚æœæœ‰é—®ç­”ï¼Œæ˜ç¡®æ ‡å‡ºé—®é¢˜æ˜¯å¦å¾—åˆ°è§£ç­”

ç›´æ¥è¾“å‡ºæ€»ç»“ï¼Œæ— éœ€é¢å¤–è¯´æ˜ã€‚{context_info}"""

            # è°ƒç”¨LLM
            response = await self.client.chat.completions.create(
                model=settings.llm_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            summary = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0
            
            logger.info(f"Generated summary, tokens used: {tokens_used}")
            
            return {
                "summary": summary,
                "tokens_used": tokens_used,
                "model": settings.llm_model
            }
            
        except OpenAIError as e:
            logger.error(f"OpenAI API error: {e}")
            return None
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return None
    
    async def generate_daily_digest(
        self,
        messages: List[Dict[str, str]],
        date_str: str,
        stats: Optional[Dict] = None
    ) -> Optional[Dict[str, any]]:
        """
        ç”Ÿæˆæ¯æ—¥æ‘˜è¦
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ "2026-01-16"
            stats: ç»Ÿè®¡æ•°æ® {"total_messages": 100, "active_users": 20, ...}
            
        Returns:
            {"summary": "æ‘˜è¦æ–‡æœ¬", "tokens_used": tokenæ•°} æˆ– None
        """
        if not self.is_enabled:
            return None
            
        context = f"è¿™æ˜¯{date_str}çš„ç¾¤èŠè®°å½•"
        if stats:
            context += f"ï¼Œå…±{stats.get('total_messages', 0)}æ¡æ¶ˆæ¯ï¼Œ{stats.get('active_users', 0)}ä½æ´»è·ƒæˆå‘˜"
        
        # ä½¿ç”¨æ›´å¤§çš„tokené™åˆ¶ç”¨äºæ¯æ—¥æ‘˜è¦
        return await self.summarize_messages(messages, context=context, max_tokens=1500)
    
    async def health_check(self) -> bool:
        """æ£€æŸ¥LLMæœåŠ¡æ˜¯å¦å¯ç”¨"""
        if not self.is_enabled:
            return False
        
        try:
            # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
            response = await self.client.chat.completions.create(
                model=settings.llm_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=5
            )
            return True
        except Exception as e:
            logger.error(f"LLM health check failed: {e}")
            return False


# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()
